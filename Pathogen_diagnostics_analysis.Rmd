---
title: "Pathogen Diagnostics using Nanopore MinIon"
subtitle: "Oxford Nanopore sequencing technology as a rapid, portable and accurate tool for fungal pathogen diagnostics in chickpea"
author: "Ido Bar & Oak Hatzimanolis"
date: "17 May 2019"
always_allow_html: yes
output: 
    # md_document:
#      css: "style/style.css"
      # toc: true
      # toc_depth: 3
#      highlight: pygments
#      number_sections: false
    html_document:
      css: "style/style.css"
      toc: true
      toc_float: true
      toc_depth: 3
      highlight: pygments
      number_sections: false
      code_folding: hide
#      keep_md: true
bibliography: style/Pathogen_diagnostics.bib
csl: style/springer-basic-improved-author-date-with-italic-et-al-period.csl
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(list(echo = TRUE, eval=FALSE, message=FALSE))
# options(width = 180)
cran_packages <- c("tidyverse", "knitr", "pander", "captioner", "DT", "kableExtra", "paletteer")
pacman::p_load(char=cran_packages, repos="https://cran.rstudio.com/")
devtools::source_gist("7f63547158ecdbacf31b54a58af0d1cc", filename = "util.R")
# Connect to Zotero to access references
# biblio <- ReadBib("data/Fungal_genomes.bib") # , "bibtex", "RefManageR"
# DataTable options
options(DT.options = list(pageLength = 15, searching = FALSE))
# Font Format
custom_font="consolas"
fontFmt = function(x,font="consolas"){
  #outputFormat = knitr::opts_knit$get("rmarkdown.pandoc.to")
  #if (outputFormat == 'html')
  formatted_text <- sprintf("<font face='%s'>%s</font>",font,x)
  return(formatted_text)
  #else
  #  x
}
```



```{r captions, include=FALSE, eval=TRUE}
figs <- captioner(prefix="Figure")
tbls <- captioner(prefix="Table")
tbls(name="samples","Quality check of DNA samples used for Nanopore sequencing.")
tbls(name="barcoding", "Summary of barcoding with guppy v3.0.3 and Deepbinner.")

sample_table <- readxl::read_excel("./sample_info/Oak_DNA_extractions.xlsx", sheet = "DNA_QC")

```

# Objective
Develop highly specific, real-time chickpea fungal pathogens detection and analysis pipeline based on long-read Nanopore DNA sequencing 

# Aims
1.	Establish and optimise the methods and protocols for reproducible extraction of high molecular weight DNA suitable for long-read whole-genome-sequencing from fungal cultures and inoculated plant samples
2.	Develop target species' databases that will be used as references during sequencing (_Ascochyta rabiei_, _Botrytis cinerea_ and potentially other closely related species)
3.	Accurately identify pathogens from mixed samples from controlled experiments and actual infected chickpea from growing paddocks (down to isolate-level resolution) 

# Experimental Design 
## Methods
* Extraction of high molecular weight pure fungal DNA suitable for long-read whole-genome-sequencing  
* Oxford Nanopore long-read sequencing, QC and taxonomic classification  
* Pathogen identification and abundance estimation  

Details of the sequenced samples is provided in (`r tbls(name="samples",display="cite")`).

```{r samples_table, eval=TRUE} 
datatable(as.data.frame(sample_table), caption=tbls("samples"), rownames = FALSE)# pander , justify="left"
```


DNA-Seq data processing, and taxonomic classifications were performed on the _Griffith University Gowonda HPC Cluster_ (using Torque scheduler), following the methods specified by ....  

Detailed methods, including code for running each of the analyses steps are provided in the associated [Pathogen_diagnostics_analysis GitHub repository](https://github.com/IdoBar/Pathogen_diagnostics_analysis).

### Computing Environment Praparation
Install needed software in a `conda` environment on Gowonda2.
```{bash setup_tools}
# install and setup conda environment (maybe a new one for using the GPU node) (need to fix internet connection to gowonda2 - use patched netcheck in ~/bin)
~/bin/netcheck
conda create -y -n kraken -c bioconda kraken2 bracken krona
# Clean extra space
conda clean -y --all
# install deepbinner in a new conda environment
conda create -y --name tf_gpu -c bioconda tensorflow-gpu deepbinner ont-fast5-api
# Install pdfx to parse the report and download the files, see https://stackoverflow.com/a/33173484
easy_install -U pdfx
```

Then download and setup pre-made reference databases for Kraken2 [@Wood_Kraken_2014].
```{bash setup_kraken}
NCORES=12
# define location for Kraken databases
KRAKEN_DB=/project/aerc/ncbi/kraken_dbs/pathogen_diag_refs
# activate kraken environment
conda activate kraken
# Create a new folder for the databases
mkdir -p $KRAKEN_DB
# download taxonomy db from NCBI
kraken2-build --download-taxonomy --db $KRAKEN_DB --threads $NCORES
# download nucleotide databases from NCBI
kraken2-build --download-library fungi --db $KRAKEN_DB --threads $NCORES
kraken2-build --download-library bacteria --db $KRAKEN_DB --threads $NCORES
kraken2-build --download-library viral --db $KRAKEN_DB --threads $NCORES
kraken2-build --download-library plant --db $KRAKEN_DB --threads $NCORES
```
Download custom genomes and prepare as references, following the instructions in the [Kraken2 manual](https://ccb.jhu.edu/software/kraken2/index.shtml?t=manual#custom-databases):

>Sequences not downloaded from NCBI may need their taxonomy information assigned explicitly. This can be done using the string kraken:taxid|XXX in the sequence ID, with XXX replaced by the desired taxon ID.

```{bash custom_refs}
mkdir -p /project/aerc/ncbi/kraken_dbs/custom_genomes
cd !$
# B. fabae
wget ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/004/335/055/GCA_004335055.1_Bfabae_DLY-16-612/GCA_004335055.1_Bfabae_DLY-16-612_genomic.fna.gz
# A. rabiei
wget ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/004/011/695/GCA_004011695.1_Arabiei_Me14/GCA_004011695.1_Arabiei_Me14_genomic.fna.gz
# B. cinerea
wget ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/143/535/GCA_000143535.4_ASM14353v4/GCA_000143535.4_ASM14353v4_genomic.fna.gz
wget ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/003/497/985/GCA_003497985.1_ASM349798v1/GCA_003497985.1_ASM349798v1_genomic.fna.gz
wget ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/900/232/045/GCA_900232045.1_PTT_W1-1/GCA_900232045.1_PTT_W1-1_genomic.fna.gz
wget ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/900/231/935/GCA_900231935.1_Ptm_SG1/GCA_900231935.1_Ptm_SG1_genomic.fna.gz
wget ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/003/171/515/GCA_003171515.1_ASM317151v1/GCA_003171515.1_ASM317151v1_genomic.fna.gz
wget ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/001/857/865/GCA_001857865.1_ASM185786v1/GCA_001857865.1_ASM185786v1_genomic.fna.gz
wget ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/002/896/005/GCA_002896005.2_CDCFrontier_v0.5/GCA_002896005.2_CDCFrontier_v0.5_genomic.fna.gz
# create commands to add taxonomy ids
ls -1 *.fna.gz | parallel --dry-run "pigz -cd {} | sed -E '/>/s/(>[^ ]+)/\1|kraken:taxid|XX/g' > {.}" > add_tax_ids.bash
# manually edit th efile and change the XX with each species' taxid
nano add_tax_ids.bash
pigz -cd GCA_000143535.4_ASM14353v4_genomic.fna.gz sed -E '/>/s/(>[^ ]+)/\1|kraken:taxid|40559|/g' > GCA_000143535.4_ASM14353v4_genomic.fna
pigz -cd GCA_001857865.1_ASM185786v1_genomic.fna.gz sed -E '/>/s/(>[^ ]+)/\1|kraken:taxid|5180/g' > GCA_001857865.1_ASM185786v1_genomic.fna
pigz -cd GCA_002896005.2_CDCFrontier_v0.5_genomic.fna.gz sed -E '/>/s/(>[^ ]+)/\1|kraken:taxid|3827/g' > GCA_002896005.2_CDCFrontier_v0.5_genomic.fna
pigz -cd GCA_003171515.1_ASM317151v1_genomic.fna.gz sed -E '/>/s/(>[^ ]+)/\1|kraken:taxid|45151/g' > GCA_003171515.1_ASM317151v1_genomic.fna
pigz -cd GCA_003497985.1_ASM349798v1_genomic.fna.gz sed -E '/>/s/(>[^ ]+)/\1|kraken:taxid|13684/g' > GCA_003497985.1_ASM349798v1_genomic.fna
pigz -cd GCA_004011695.1_Arabiei_Me14_genomic.fna.gz sed -E '/>/s/(>[^ ]+)/\1|kraken:taxid|5454/g' > GCA_004011695.1_Arabiei_Me14_genomic.fna
pigz -cd GCA_004335055.1_Bfabae_DLY-16-612_genomic.fna.gz sed -E '/>/s/(>[^ ]+)/\1|kraken:taxid|182092/g' > GCA_004335055.1_Bfabae_DLY-16-612_genomic.fna
pigz -cd GCA_900231935.1_Ptm_SG1_genomic.fna.gz sed -E '/>/s/(>[^ ]+)/\1|kraken:taxid|97480/g' > GCA_900231935.1_Ptm_SG1_genomic.fna
pigz -cd GCA_900232045.1_PTT_W1-1_genomic.fna.gz sed -E '/>/s/(>[^ ]+)/\1|kraken:taxid|97479/g' > GCA_900232045.1_PTT_W1-1_genomic.fna
pigz -cd GCF_000143535.2_ASM14353v4_genomic.fna.gz sed -E '/>/s/(>[^ ]+)/\1|kraken:taxid|40559/g' > GCF_000143535.2_ASM14353v4_genomic.fna
# Add the files to Kraken database
RUN=add_custom_refs
NCORES=12
parallel --dry-run " kraken2-build --add-to-library {} --db $KRAKEN_DB --threads $NCORES" ::: *.fna > ${RUN}.bash
# Prepare PBS script
echo '#!/bin/bash -v
#PBS -V
#PBS -l' "select=1:ncpus=$NCORES:mem=16GB,walltime=10:30:00

cd \$PBS_O_WORKDIR
conda activate kraken
gawk -v ARRAY_IND=\$PBS_ARRAY_INDEX 'NR==ARRAY_IND' \${CMDS_FILE} | bash" > ${RUN}.pbspro

# Run the commands 
JOBS_NUM=`wc -l ${RUN}.bash | gawk '{print $1}'`
CUSTOM_REFS_ID=$( qsub -J1-$JOBS_NUM -N ${RUN:0:11} -vCMDS_FILE=${RUN}.bash  ${RUN}.pbspro ) 
CUSTOM_REFS_ID=$( echo $CUSTOM_REFS_ID | egrep -o "[0-9]{7}"  )

# find and remove empty files
find . -size 0 -exec rm {} + 

# Check that all jobs finished successfuly
find . -regextype posix-egrep -regex '\./.*\.e[0-9]{7}.*' | xargs grep "ExitStatus" 

#
```
Build reference database library (took 2h:15m and requires >76Gb RAM with 16 cores)
```{bash kraken_build}
NCORES=12
# define location for Kraken databases
KRAKEN_DB=/project/aerc/ncbi/kraken_dbs/pathogen_diag_refs
# send the job
echo "cd \$PBS_O_WORKDIR ; conda activate kraken; kraken2-build --build --db $KRAKEN_DB --threads $NCORES" | qsub -V -l select=1:ncpus=$NCORES:mem=96GB,walltime=5:00:00 -N kraken_build # 5313443.pbsserver
# inspect the resulting database

echo "cd \$PBS_O_WORKDIR ; conda activate kraken; kraken2-inspect --db $KRAKEN_DB --threads $NCORES" | qsub -V -l select=1:ncpus=$NCORES:mem=96GB,walltime=00:30:00 -N kraken_inspect # 5313862.pbsserver

```


### Nanopore Sequencing

#### Basecalling
The raw reads (in `.fast5` format) produced by MinKnow where copied to the *Gowonda HPC cluster* (Griffith University) and the bases were called into `.fastq` files using Guppy v3.0.3+7e7b7d0, utilising a GPU-enabled node on the cluster, equipped with a Tesla V100-PCIE-32GB GPU. The process was substantially faster than based on CPUs and completed within 24 minutes.
```{bash guppy_basecall}
# log into the GPU node
ssh n060
# Create a folder on the GPU node
mkdir -p ~/lscratch/Nanopore_data/Pooled_inoculated_chickpea
# copy all the .fast5 files from scratch to lscratch
find ~/scratch/data/Nanopore_data/Oak_diagnostics/Oak_pathogen_detection/Pooled_inoculated_chickpea/20190514_0639_MN30244_FAK12021_555b1a0a/ -name "*.fast5" -exec cp {} ./Pooled_inoculated_chickpea/ +
# go back to login node
exit
# Send the guppy_basecaller command to the GPU node
echo "cd \$PBS_O_WORKDIR ; module conda ; ~/etc/tools/Nanopore/ont-guppy/bin/guppy_basecaller --flowcell FLO-MIN106 --kit SQK-RBK004 --input_path ~/lscratch/Nanopore_data/Pooled_inoculated_chickpea --save_path ~/lscratch/Nanopore_data/guppy_gpu_basecall -x 'cuda:0'" | qsub -V -l select=1:ncpus=1:ngpus=1:mem=24GB,walltime=10:00:00 -W group_list=deeplearning -A deeplearning -q dljun@n060 -N guppy_gpu
# After completion
# log back into the GPU node
ssh n060
# copy results back
cp -r ~/lscratch/Nanopore_data/guppy_gpu_basecall ~/scratch/data/Nanopore_data/Oak_diagnostics/
exit
```
#### Demultiplex Reads
The resulting `.fastq` files of reads that passed QC were then combined and demultiplexed based on their barcodes to seperate files representing the reads from each DNA library. The resources by guppy to perform the barcode classifications were a mere 15Mb RAM and 2 min when utilising 16 cores. 
```{bash demultiplexing}
# demultiplex results
NCORES=12
cd ~/scratch/data/Nanopore_data/Oak_diagnostics/
echo "cd \$PBS_O_WORKDIR ; ~/etc/tools/Nanopore/ont-guppy/bin/guppy_barcoder --barcode_kits SQK-RBK004 -q 0 -i ~/scratch/data/Nanopore_data/Oak_diagnostics/guppy_gpu_basecall -s ~/scratch/data/Nanopore_data/Oak_diagnostics/guppy_gpu_basecall/demultiplexed_fastq -t $NCORES" | qsub -V -l select=1:ncpus=$NCORES:mem=48GB,walltime=1:00:00 -N guppy_barcode # 5313461.pbsserver
```
An alternative to demultiplexing with guppy is to use deepbinner [@Wick_Deepbinner_2018], which applies a deep convolutional neural network on the raw signal (this can be done in real-time while sequencing) to classify the reads to their respective barcode. Running deepbinner after the MinIon run had ended required that the multi-read fast5 files were first split to single-read files. This was achieved using the `multi_to_single_fast5` utility from [ont_fast5_api repository](https://github.com/nanoporetech/ont_fast5_api), utilising 16 cores, 5.4Gb RAM and a runtime of 21 minutes. The single-read files were then analysed to assign the read to a barcode using the `deepbinner classify` command, followed by a second `deepbinner bin` command to separate the fastq reads according to the determined classification. The performance of deepbinner can be substantially enhanced using GPU processing and therfore it was run on the same GPU-enabled node mentioned earlier (runtime was 3h47min and 5min for both processes, respectively).      
```{bash deepbinner}
# demultiplex with Deepbinner
# Split the multi-read fast5 files to single-read
NCORES=12
echo "cd \$PBS_O_WORKDIR ; conda activate tf_gpu; multi_to_single_fast5 --recursive -i ~/scratch/data/Nanopore_data/Oak_diagnostics/Oak_pathogen_detection/Pooled_inoculated_chickpea/20190514_0639_MN30244_FAK12021_555b1a0a -s ~/scratch/data/Nanopore_data/Oak_diagnostics/Oak_pathogen_detection/Pooled_inoculated_chickpea/single-read_fast5 -t $NCORES" | qsub -V -l select=1:ncpus=$NCORES:mem=96GB,walltime=5:00:00  # 5313469.pbsserv
rsync -azr ~/scratch/data/Nanopore_data/Oak_diagnostics/Oak_pathogen_detection/Pooled_inoculated_chickpea/single-read_fast5 n060:~/lscratch/Nanopore_data/Oak_pathogen_diagnostics/
# classifying fast5 reads
DEEPBIN_ID=$( echo "cd ~/lscratch/Nanopore_data/Oak_pathogen_diagnostics/ ; source ~/.bashrc; conda activate tf_gpu; deepbinner classify --rapid ~/lscratch/Nanopore_data/Oak_pathogen_diagnostics/single-read_fast5 > classifications " | qsub -V -l select=1:ncpus=1:ngpus=1:mem=24GB,walltime=10:00:00 -W group_list=deeplearning -A deeplearning -q dljun@n060 -N deepbin_class ) # 664.n060
# DEEPBIN_ID=$( echo $DEEPBIN_ID | egrep -o "[0-9]+"  )

# binning basecalled reads
ls ~/scratch/data/Nanopore_data/Oak_diagnostics/guppy_gpu_basecall/*.fastq | parallel cat | pigz > ~/scratch/data/Nanopore_data/Oak_diagnostics/guppy_gpu_basecall.fastq.gz 
rsync -azr ~/scratch/data/Nanopore_data/Oak_diagnostics/guppy_gpu_basecall.fastq.gz n060:~/lscratch/Nanopore_data/Oak_pathogen_diagnostics/
echo "cd ~/lscratch/Nanopore_data/Oak_pathogen_diagnostics/; source ~/.bashrc; conda activate tf_gpu; deepbinner bin --classes classifications --reads ~/lscratch/Nanopore_data/Oak_pathogen_diagnostics/guppy_gpu_basecall.fastq.gz --out_dir ~/lscratch/Nanopore_data/Oak_pathogen_diagnostics/binned_reads" | qsub -V -l select=1:ncpus=1:ngpus=1:mem=24GB,walltime=10:00:00 -W group_list=deeplearning -W depend=afterok:$DEEPBIN_ID -A deeplearning -q dljun@n060 -N deepbin_bin

```

```{r barcoding, eval=TRUE}
guppy_dedup <- read_tsv("./data/guppy_barcoding_summary.txt" ) # %>% 
  # count(barcode_arrangement) %>% rename(Barcode=barcode_arrangement, guppy_count=n) %>% 
  # mutate(Barcode=sub("barcode", "", Barcode),
  #        guppy_prop=guppy_count/sum(guppy_count)) %>% write_tsv("./data/guppy_barcoding_summary.txt")
deepbinner_class <- read_tsv("./data/deepbinner_classification.txt") %>%
  mutate(deepbinner_prop=Count/sum(Count)) %>% rename(deepbinner_count=Count) 
dedup_summary <- deepbinner_class  %>% 
  inner_join(guppy_dedup) %>% mutate(count_diff=deepbinner_count - guppy_count)
mean_diff <- dedup_summary %>% filter(guppy_count>500, Barcode!="unclassified") %>% 
   summarise(mean(count_diff)) 
class_diff <- dedup_summary %>% filter(Barcode=="unclassified")
  
```
Overall, deepbinner was able to assign more reads into their barcodes than guppy (in average `r mean_diff` more reads), at the expense of some false positives (as can be seen for barcodes 06-12 in `r tbls(name="barcoding",display="cite")`) and substantially increased runtime and computing resources. (point for discussion - are the extra resources needed for diagnostics? probably not...)

```{r dedup_table, eval=TRUE} 
datatable(dedup_summary, caption=tbls("barcoding"), rownames = FALSE) %>% 
  formatPercentage(c('deepbinner_prop', 'guppy_prop'), 2)# pander , justify="left"
```
### Assign Taxa
Reads were assigned to their taxonomic origin using Kraken2 v2.0.8-beta, requiring 42-75Gb RAM and 1-13.5 min processing time using 12 cores for each separate barcode file, depending on the number of reads it contained. The assigned taxonomic classes were further investigated and compared between the samples using Pavian v0.1.3 [@Breitwieser_Pavian_2016].
```{bash kraken2}
mkdir -p $HOME/scratch/data/Nanopore_data/Oak_diagnostics/kraken_taxonomy/fastq
# symlink files into the folder
RUN=kraken2_tax
NCORES=12
# define location for Kraken databases
KRAKEN_DB=/project/aerc/ncbi/kraken_dbs/pathogen_diag_refs
# create the kraken jobs (to read hash table from disk use `--memory-mapping`)
cd $HOME/scratch/data/Nanopore_data/Oak_diagnostics/kraken_taxonomy
ls -1 $HOME/scratch/data/Nanopore_data/Oak_diagnostics/kraken_taxonomy/fastq/*.fastq | parallel --dry-run "kraken2 --db $KRAKEN_DB --threads $NCORES --use-names --use-mpa-style --report  {/.}_kraken2_mpa_report.txt {} > {/.}_kraken2_mpa_output.txt" > ${RUN}.bash
# Prepare PBS script
echo '#!/bin/bash -v
#PBS -V
#PBS -l' "select=1:ncpus=$NCORES:mem=72GB,walltime=10:30:00

cd \$PBS_O_WORKDIR
conda activate kraken
gawk -v ARRAY_IND=\$PBS_ARRAY_INDEX 'NR==ARRAY_IND' \${CMDS_FILE} | bash" > ${RUN}.pbspro

# Run the commands 
JOBS_NUM=`wc -l ${RUN}.bash | gawk '{print $1}'`
KRAKEN2_ID=$( qsub -J1-$JOBS_NUM -N ${RUN:0:11} -vCMDS_FILE=${RUN}.bash  ${RUN}.pbspro ) 
KRAKEN2_ID=$( echo $KRAKEN2_ID | egrep -o "[0-9]{7}"  )
# find and remove empty files
find . -size 0 -exec rm {} + 

# Check that all jobs finished successfuly
find . -regextype posix-egrep -regex '\./.*\.e[0-9]{7}.*' | xargs grep "ExitStatus" 

```

```{r summarise_taxa, eval=TRUE}
barcode_dict <- setNames(c("9 dpi", "0 hpi", "48 hpi (inoculated)", "48 hpi (control)", 
                           "48 hpi (sterilised)", "Unclassified"), c(paste0("barcode0", 1:5), "unclassified") )
# Kraken2 report files
kraken_files <- list.files("./data/kraken_taxonomy/", "kraken2_report.txt", full.names = TRUE)
# read in report files
for (f in kraken_files){
  # f=kraken_files[2]
  taxa_report <- read_tsv(f, col_names = c("Coverage_perc", "Rooted_reads", "Taxa_reads", 
                                            "Rank", "NCBI_txid", "Scientific_name")) %>% 
    mutate(Barcode=sub("_kraken2_report.txt", "", basename(f)), Sample=barcode_dict[Barcode])
  phyla_data <- taxa_report %>% filter(Rank=="P", Rooted_reads>5) %>%
    arrange(desc(Rooted_reads))
  genus_data <- taxa_report %>% filter(Rank=="G", Rooted_reads>5) %>%
    arrange(desc(Rooted_reads))
  species_data <- taxa_report %>% filter(Rank=="S", Rooted_reads>5) %>%
    arrange(desc(Rooted_reads))
    # mutate(Scientific_name=fct_lump_min(Scientific_name, 4))
}


# 
# 
# all_taxa <- kraken_files %>% purrr::map_dfr(~read_tsv(.x, 
#                           col_names = c("Coverage_perc", "Rooted_reads", "Taxa_reads", 
#                                             "Rank", "NCBI_txid", "Scientific_name")) %>% 
#                             mutate(barcode=sub("_kraken2_report.txt", "", basename(.x))))
# plot_data <- all_taxa %>% filter(Rank=="P", Rooted_reads>5, 
#                                  Scientific_name!="Streptophyta") %>%
#   mutate(Scientific_name=fct_lump(Scientific_name, 8))
# p <- ggplot(plot_data, 
#             aes(x=barcode, y=Rooted_reads, fill=Scientific_name))
# p + geom_bar(stat = 'identity', width=0.5) + scale_fill_d3() +
#   # plot_theme(baseSize = 20) + 
#   NULL
# # top_species <- 
#   taxa_report %>% filter(Rank=="P") %>% filter(Rooted_reads>5) %>% arrange(desc(Rooted_reads))
  

# count(NCBI_txid, Scientific_name) %>% arrange(desc(n))
```

## General information
This document was last updated at `r Sys.time()` using R Markdown (built with `r R.version.string`). Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. It is especially powerful at authoring documents and reports which include code and can execute code and use the results in the output. For more details on using R Markdown see <http://rmarkdown.rstudio.com> and [Rmarkdown cheatsheet](https://www.rstudio.com/wp-content/uploads/2016/03/rmarkdown-cheatsheet-2.0.pdf).

***
## Bibliography

<!-- ```{r results='asis', eval=TRUE} -->
<!-- PrintBibliography(biblio) -->
<!-- ``` -->

