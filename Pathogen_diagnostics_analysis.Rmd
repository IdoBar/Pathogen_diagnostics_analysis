---
title: "Pathogen Diagnostics using Nanopore MinIon"
subtitle: "Oxford Nanopore sequencing technology as a rapid, portable and accurate tool for fungal pathogen diagnostics in chickpea"
author: "Ido Bar & Oak Hatzimanolis"
date: "17 May 2019"
always_allow_html: yes
output: 
    # md_document:
#      css: "style/style.css"
      # toc: true
      # toc_depth: 3
#      highlight: pygments
#      number_sections: false
    html_document:
      css: "style/style.css"
      toc: true
      toc_float: true
      toc_depth: 3
      highlight: pygments
      number_sections: false
      code_folding: hide
#      keep_md: true
bibliography: style/Pathogen_diagnostics.bib
csl: style/springer-basic-improved-author-date-with-italic-et-al-period.csl
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(list(echo = TRUE, eval=FALSE, message=FALSE))
# options(width = 180)
cran_packages <- c("tidyverse", "knitr", "pander", "captioner", "DT", "kableExtra")
pacman::p_load(char=cran_packages, repos="https://cran.rstudio.com/")
# Connect to Zotero to access references
# biblio <- ReadBib("data/Fungal_genomes.bib") # , "bibtex", "RefManageR"
# DataTable options
options(DT.options = list(pageLength = 15, searching = FALSE))
# Font Format
custom_font="consolas"
fontFmt = function(x,font="consolas"){
  #outputFormat = knitr::opts_knit$get("rmarkdown.pandoc.to")
  #if (outputFormat == 'html')
  formatted_text <- sprintf("<font face='%s'>%s</font>",font,x)
  return(formatted_text)
  #else
  #  x
}
```



```{r captions, include=FALSE, eval=TRUE}
figs <- captioner(prefix="Figure")
tbls <- captioner(prefix="Table")
tbls(name="samples","Quality check of DNA samples used for Nanopore sequencing.")
tbls(name="barcoding", "Summary of barcoding with guppy v3.0.3 and Deepbinner.")

sample_table <- readxl::read_excel("./sample_info/Oak_DNA_extractions.xlsx", sheet = "DNA_QC")

```

# Objective
Develop highly specific, real-time chickpea fungal pathogens detection and analysis pipeline based on long-read Nanopore DNA sequencing 

# Aims
1.	Establish and optimise the methods and protocols for reproducible extraction of high molecular weight DNA suitable for long-read whole-genome-sequencing from fungal cultures and inoculated plant samples
2.	Develop target species' databases that will be used as references during sequencing (_Ascochyta rabiei_, _Botrytis cinerea_, _Botrytis fabae_ and potentially other closely related species)
3.	Accurately identify pathogens from mixed samples from controlled experiments and actual infected chickpea from growing paddocks (down to isolate-level resolution) 

# Experimental Design 
## Methods
* Extraction of high molecular weight pure fungal DNA suitable for long-read whole-genome-sequencing  
* Oxford Nanopore long-read sequencing, QC and taxonomic classification  
* Pathogen identification and abundance estimation  

Details of the sequenced samples is provided in (`r tbls(name="samples",display="cite")`).

```{r samples_table, eval=TRUE} 
datatable(as.data.frame(sample_table), caption=tbls("samples"), rownames = FALSE)# pander , justify="left"
```


DNA-Seq data processing, and taxonomic classifications were performed on the _Griffith University Gowonda HPC Cluster_ (using Torque scheduler), following the methods specified by ....  

Detailed methods, including code for running each of the analyses steps are provided in the associated [Pathogen_diagnostics_analysis GitHub repository](https://github.com/IdoBar/Pathogen_diagnostics_analysis).

### Computing Environment Praparation
Install needed software in a `conda` environment on Gowonda2.
```{bash setup_tools}
# install and setup conda environment (maybe a new one for using the GPU node) (need to fix internet connection to gowonda2 - use patched netcheck in ~/bin)
~/bin/netcheck
conda create --name tf_gpu -c bioconda tensorflow-gpu blast kraken2 deepbinner
conda install -c bioconda blast kraken2 deepbinner
conda install -c biobuilds igv
# Clean extra space
conda clean -y --all
# Install pdfx to parse the report and download the files, see https://stackoverflow.com/a/33173484
easy_install -U pdfx
```

Then download and setup pre-made reference databases for Kraken2 [@Wood_Kraken_2014].
```{bash setup_kraken}
NCORES=12
# define location for Kraken databases
KRAKEN_DB=/project/aerc/ncbi/kraken_dbs/pathogen_diag_refs
# Create a new folder for the databases
mkdir -p $KRAKEN_DB
# download taxonomy db from NCBI
kraken2-build --download-taxonomy --db $KRAKEN_DB --threads $NCORES
# download nucleotide databases from NCBI
kraken2-build --download-library fungi --db $KRAKEN_DB --threads $NCORES
kraken2-build --download-library bacteria --db $KRAKEN_DB --threads $NCORES
kraken2-build --download-library viral --db $KRAKEN_DB --threads $NCORES
kraken2-build --download-library plant --db $KRAKEN_DB --threads $NCORES
```
Download custom genomes and prepare as references, following the instructions in the [Kraken2 manual](https://ccb.jhu.edu/software/kraken2/index.shtml?t=manual#custom-databases):
>Sequences not downloaded from NCBI may need their taxonomy information assigned explicitly. This can be done using the string kraken:taxid|XXX in the sequence ID, with XXX replaced by the desired taxon ID.

```{bash custom_refs}
mkdir -p /project/aerc/ncbi/kraken_dbs/custom_genomes
cd !$
# B. fabae
wget ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/004/335/055/GCA_004335055.1_Bfabae_DLY-16-612/GCA_004335055.1_Bfabae_DLY-16-612_genomic.fna.gz
# A. rabiei
wget ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/004/011/695/GCA_004011695.1_Arabiei_Me14/GCA_004011695.1_Arabiei_Me14_genomic.fna.gz
# B. cinerea
wget ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/143/535/GCA_000143535.4_ASM14353v4/GCA_000143535.4_ASM14353v4_genomic.fna.gz
wget ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/003/497/985/GCA_003497985.1_ASM349798v1/GCA_003497985.1_ASM349798v1_genomic.fna.gz
wget ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/900/232/045/GCA_900232045.1_PTT_W1-1/GCA_900232045.1_PTT_W1-1_genomic.fna.gz
wget ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/900/231/935/GCA_900231935.1_Ptm_SG1/GCA_900231935.1_Ptm_SG1_genomic.fna.gz
wget ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/003/171/515/GCA_003171515.1_ASM317151v1/GCA_003171515.1_ASM317151v1_genomic.fna.gz
wget ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/001/857/865/GCA_001857865.1_ASM185786v1/GCA_001857865.1_ASM185786v1_genomic.fna.gz
wget ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/002/896/005/GCA_002896005.2_CDCFrontier_v0.5/GCA_002896005.2_CDCFrontier_v0.5_genomic.fna.gz
# create commands to add taxonomy ids
ls -1 *.fna.gz | parallel --dry-run "pigz -cd {} | sed -E '/>/s/(>[^ ]+)/\1|kraken:taxid|XX/g' > {.}" > add_tax_ids.bash
# manually edit th efile and change the XX with each species' taxid
nano add_tax_ids.bash
pigz -cd GCA_000143535.4_ASM14353v4_genomic.fna.gz sed -E '/>/s/(>[^ ]+)/\1|kraken:taxid|40559|/g' > GCA_000143535.4_ASM14353v4_genomic.fna
pigz -cd GCA_001857865.1_ASM185786v1_genomic.fna.gz sed -E '/>/s/(>[^ ]+)/\1|kraken:taxid|5180/g' > GCA_001857865.1_ASM185786v1_genomic.fna
pigz -cd GCA_002896005.2_CDCFrontier_v0.5_genomic.fna.gz sed -E '/>/s/(>[^ ]+)/\1|kraken:taxid|3827/g' > GCA_002896005.2_CDCFrontier_v0.5_genomic.fna
pigz -cd GCA_003171515.1_ASM317151v1_genomic.fna.gz sed -E '/>/s/(>[^ ]+)/\1|kraken:taxid|45151/g' > GCA_003171515.1_ASM317151v1_genomic.fna
pigz -cd GCA_003497985.1_ASM349798v1_genomic.fna.gz sed -E '/>/s/(>[^ ]+)/\1|kraken:taxid|13684/g' > GCA_003497985.1_ASM349798v1_genomic.fna
pigz -cd GCA_004011695.1_Arabiei_Me14_genomic.fna.gz sed -E '/>/s/(>[^ ]+)/\1|kraken:taxid|5454/g' > GCA_004011695.1_Arabiei_Me14_genomic.fna
pigz -cd GCA_004335055.1_Bfabae_DLY-16-612_genomic.fna.gz sed -E '/>/s/(>[^ ]+)/\1|kraken:taxid|182092/g' > GCA_004335055.1_Bfabae_DLY-16-612_genomic.fna
pigz -cd GCA_900231935.1_Ptm_SG1_genomic.fna.gz sed -E '/>/s/(>[^ ]+)/\1|kraken:taxid|97480/g' > GCA_900231935.1_Ptm_SG1_genomic.fna
pigz -cd GCA_900232045.1_PTT_W1-1_genomic.fna.gz sed -E '/>/s/(>[^ ]+)/\1|kraken:taxid|97479/g' > GCA_900232045.1_PTT_W1-1_genomic.fna
pigz -cd GCF_000143535.2_ASM14353v4_genomic.fna.gz sed -E '/>/s/(>[^ ]+)/\1|kraken:taxid|40559/g' > GCF_000143535.2_ASM14353v4_genomic.fna
# Add the files to Kraken database
RUN=add_custom_refs
NCORES=12
parallel --dry-run "kraken2-build --add-to-library {$file} --db $KRAKEN_DB --threads $NCORES" ::: *.fna > ${RUN}.bash
# Prepare PBS script
echo '#!/bin/bash -v
#PBS -V
#PBS -l' "select=1:ncpus=$NCORES:mem=16GB,walltime=10:30:00

cd \$PBS_O_WORKDIR
gawk -v ARRAY_IND=\$PBS_ARRAY_INDEX 'NR==ARRAY_IND' \${CMDS_FILE} | bash" > ${RUN}.pbspro

# Run the commands 
JOBS_NUM=`wc -l ${RUN}.bash | gawk '{print $1}'`
CUSTOM_REFS_ID=$( qsub -J1-$JOBS_NUM -N ${RUN:0:11} -vCMDS_FILE=${RUN}.bash  ${RUN}.pbspro ) 
CUSTOM_REFS_ID=$( echo $CUSTOM_REFS_ID | egrep -o "[0-9]{7}"  )
# find and remove empty files
find . -size 0 -exec rm {} + 

# Check that all jobs finished successfuly
find . -regextype posix-egrep -regex '\./.*\.e[0-9]{7}.*' | xargs grep "ExitStatus" 

#
```
Build reference database library (took 2h:15m and requires >76Gb RAM)
```{bash kraken_build}
NCORES=16
# define location for Kraken databases
KRAKEN_DB=/project/aerc/ncbi/kraken_dbs/pathogen_diag_refs
# send the job
echo "cd \$PBS_O_WORKDIR ; kraken2-build --build --db $KRAKEN_DB --threads $NCORES" | qsub -V -l select=1:ncpus=$NCORES:mem=96GB,walltime=5:00:00 -N kraken_build # 5313443.pbsserver

```


### Nanopore Sequencing

#### Basecall
The raw reads (in `.fast5` format) produced by MinKnow where copied to the *Gowonda HPC cluster* (Griffith University) and the bases were called into `.fastq` files using Guppy v3.0.3+7e7b7d0, utilising a GPU-enabled node on the cluster. The process was substantially faster than based on CPUs and completed within 24 minutes.
```{bash guppy_basecall}
# log into the GPU node
ssh n060
# Create a folder on the GPU node
mkdir -p ~/lscratch/Nanopore_data/Pooled_inoculated_chickpea
# copy all the .fast5 files from scratch to lscratch
find ~/scratch/data/Nanopore_data/Oak_diagnostics/Oak_pathogen_detection/Pooled_inoculated_chickpea/20190514_0639_MN30244_FAK12021_555b1a0a/ -name "*.fast5" -exec cp {} ./Pooled_inoculated_chickpea/ +
# go back to login node
exit
# Send the guppy_basecaller command to the GPU node
echo "cd \$PBS_O_WORKDIR ; module conda ; ~/etc/tools/Nanopore/ont-guppy/bin/guppy_basecaller --flowcell FLO-MIN106 --kit SQK-RBK004 --input_path ~/lscratch/Nanopore_data/Pooled_inoculated_chickpea --save_path ~/lscratch/Nanopore_data/guppy_gpu_basecall -x 'cuda:0'" | qsub -V -l select=1:ncpus=1:ngpus=1:mem=24GB,walltime=10:00:00 -W group_list=deeplearning -A deeplearning -q dljun@n060 -N guppy_gpu
# After completion
# log back into the GPU node
ssh n060
# copy results back
cp -r ~/lscratch/Nanopore_data/guppy_gpu_basecall ~/scratch/data/Nanopore_data/Oak_diagnostics/
exit
```
#### Demultiplex Reads
The resulting `.fastq` files of reads that passed QC were then combined and demultiplexed based on their barcodes to seperate files representing the reads from each DNA sample.
```{bash demultiplexing}
# demultiplex results
NCORES=16
cd ~/scratch/data/Nanopore_data/Oak_diagnostics/
echo "cd \$PBS_O_WORKDIR ; ~/etc/tools/Nanopore/ont-guppy/bin/guppy_barcoder --barcode_kits SQK-RBK004 -q 0 -i ~/scratch/data/Nanopore_data/Oak_diagnostics/guppy_gpu_basecall -s ~/scratch/data/Nanopore_data/Oak_diagnostics/guppy_gpu_basecall/demultiplexed_fastq -t $NCORES" | qsub -V -l select=1:ncpus=$NCORES:mem=48GB,walltime=1:00:00 -N guppy_barcode # 5313461.pbsserver
```
An alternative to demultiplexing with guppy is to use deepbinner [@Wick_Deepbinner_2018], which operates on the raw reads (this can be done in real-time while sequencing)
```{bash deepbinner}
### optional ###
# demultiplex with Deepbinner
conda install --override-channels -c anaconda tensorflow-gpu
conda install --override-channels -c bioconda deepbinner
pip install ont-fast5-api
# Split the multi-read fast5 files to single-read
NCORES=32
echo "cd \$PBS_O_WORKDIR ; multi_to_single_fast5 --recursive -i ~/scratch/data/Nanopore_data/Oak_diagnostics/Oak_pathogen_detection/Pooled_inoculated_chickpea/20190514_0639_MN30244_FAK12021_555b1a0a -s ~/scratch/data/Nanopore_data/Oak_diagnostics/Oak_pathogen_detection/Pooled_inoculated_chickpea/single-read_fast5 -t $NCORES" | qsub -V -l select=1:ncpus=$NCORES:mem=96GB,walltime=5:00:00  # 5313469.pbsserv
rsync -azr ~/scratch/data/Nanopore_data/Oak_diagnostics/Oak_pathogen_detection/Pooled_inoculated_chickpea/single-read_fast5 n060:~/lscratch/Nanopore_data/Oak_pathogen_diagnostics/
# classifying fast5 reads
DEEPBIN_ID=$( echo "cd ~/lscratch/Nanopore_data/Oak_pathogen_diagnostics/ ; source ~/.bashrc; conda activate tf_gpu; deepbinner classify --rapid ~/lscratch/Nanopore_data/Oak_pathogen_diagnostics/single-read_fast5 > classifications " | qsub -V -l select=1:ncpus=1:ngpus=1:mem=24GB,walltime=10:00:00 -W group_list=deeplearning -A deeplearning -q dljun@n060 -N deepbin_class ) # 664.n060
# DEEPBIN_ID=$( echo $DEEPBIN_ID | egrep -o "[0-9]+"  )

# binning basecalled reads
ls ~/scratch/data/Nanopore_data/Oak_diagnostics/guppy_gpu_basecall/*.fastq | parallel cat | pigz > ~/scratch/data/Nanopore_data/Oak_diagnostics/guppy_gpu_basecall.fastq.gz 
rsync -azr ~/scratch/data/Nanopore_data/Oak_diagnostics/guppy_gpu_basecall.fastq.gz n060:~/lscratch/Nanopore_data/Oak_pathogen_diagnostics/
echo "cd ~/lscratch/Nanopore_data/Oak_pathogen_diagnostics/; source ~/.bashrc; conda activate tf_gpu; deepbinner bin --classes classifications --reads ~/lscratch/Nanopore_data/Oak_pathogen_diagnostics/guppy_gpu_basecall.fastq.gz --out_dir ~/lscratch/Nanopore_data/Oak_pathogen_diagnostics/binned_reads" | qsub -V -l select=1:ncpus=1:ngpus=1:mem=24GB,walltime=10:00:00 -W group_list=deeplearning -W depend=afterok:$DEEPBIN_ID -A deeplearning -q dljun@n060 -N deepbin_bin

```
Deepbinner and guppy results are summarised in (`r tbls(name="barcoding",display="cite")`).
```{r barcoding, eval=TRUE}
guppy_dedup <- read_tsv("./data/guppy_barcoding_summary.txt" ) # %>% 
  # count(barcode_arrangement) %>% rename(Barcode=barcode_arrangement, guppy_count=n) %>% 
  # mutate(Barcode=sub("barcode", "", Barcode),
  #        guppy_prop=guppy_count/sum(guppy_count)) %>% write_tsv("./data/guppy_barcoding_summary.txt")
deepbinner_class <- read_tsv("./data/deepbinner_classification.txt") %>%
  mutate(deepbinner_prop=Count/sum(Count)) %>% rename(deepbinner_count=Count) 
dedup_summary <- deepbinner_class %>% 
  inner_join(guppy_dedup)
  
```
Details of the sequenced samples is provided in (`r tbls(name="barcoding",display="cite")`).

```{r dedup_table, eval=TRUE} 
datatable(dedup_summary, caption=tbls("barcoding"), rownames = FALSE) %>% 
  formatPercentage(c('deepbinner_prop', 'guppy_prop'), 2)# pander , justify="left"
```
### Assign Taxa
```{bash kraken2}
mkdir -p $HOME/scratch/data/Nanopore_data/Oak_diagnostics/kraken_taxonomy/fastq
# symlink files into the folder
RUN=kraken2_tax
NCORES=16
# define location for Kraken databases
KRAKEN_DB=/project/aerc/ncbi/kraken_dbs/pathogen_diag_refs
# create the kraken jobs (to read hash table from disk use `--memory-mapping`)
cd $HOME/scratch/data/Nanopore_data/Oak_diagnostics/kraken_taxonomy
ls -1 $HOME/scratch/data/Nanopore_data/Oak_diagnostics/kraken_taxonomy/fastq/*.fastq | parallel --dry-run "kraken2 --db $KRAKEN_DB --threads $NCORES --use-names --report {/.}_kraken2_report.txt {} > {/.}_kraken2_output.txt" > ${RUN}.bash
# Prepare PBS script
echo '#!/bin/bash -v
#PBS -V
#PBS -l' "select=1:ncpus=$NCORES:mem=96GB,walltime=10:30:00

cd \$PBS_O_WORKDIR
gawk -v ARRAY_IND=\$PBS_ARRAY_INDEX 'NR==ARRAY_IND' \${CMDS_FILE} | bash" > ${RUN}.pbspro

# Run the commands 
JOBS_NUM=`wc -l ${RUN}.bash | gawk '{print $1}'`
KRAKEN2_ID=$( qsub -J1-$JOBS_NUM -N ${RUN:0:11} -vCMDS_FILE=${RUN}.bash  ${RUN}.pbspro ) 
KRAKEN2_ID=$( echo $KRAKEN2_ID | egrep -o "[0-9]{7}"  )
# find and remove empty files
find . -size 0 -exec rm {} + 

# Check that all jobs finished successfuly
find . -regextype posix-egrep -regex '\./.*\.e[0-9]{7}.*' | xargs grep "ExitStatus" 

```



## General information
This document was last updated at `r Sys.time()` using R Markdown (built with `r R.version.string`). Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. It is especially powerful at authoring documents and reports which include code and can execute code and use the results in the output. For more details on using R Markdown see <http://rmarkdown.rstudio.com> and [Rmarkdown cheatsheet](https://www.rstudio.com/wp-content/uploads/2016/03/rmarkdown-cheatsheet-2.0.pdf).

***
## Bibliography

<!-- ```{r results='asis', eval=TRUE} -->
<!-- PrintBibliography(biblio) -->
<!-- ``` -->

